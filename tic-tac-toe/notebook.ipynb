{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic Tac Toe DRL",
      "provenance": [],
      "mount_file_id": "https://github.com/nomomon/drl-js/blob/main/tic-tac-toe/notebook.ipynb",
      "authorship_tag": "ABX9TyPCtg4g1EolgXTAcX2IDei1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nomomon/drl-js/blob/main/tic-tac-toe/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBl0Klfol8d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTQEPi62q5i"
      },
      "source": [
        "# Game (Environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTm9-s892yVX"
      },
      "source": [
        "# 0 1 2\n",
        "# 3 4 5\n",
        "# 6 7 8\n",
        "\n",
        "def gameStatus(board):\n",
        "    lines = [\n",
        "        [0, 1, 2],\n",
        "        [3, 4, 5],\n",
        "        [6, 7, 8],\n",
        "        [0, 3, 6],\n",
        "        [1, 4, 7],\n",
        "        [2, 5, 8],\n",
        "        [0, 4, 8],\n",
        "        [2, 4, 6],\n",
        "    ]\n",
        "\n",
        "    # there is a winner\n",
        "    for line in lines:\n",
        "        if (board[line[1]] == board[line[0]] and \n",
        "            board[line[1]] == board[line[2]] and \n",
        "            board[line[1]] != 0):\n",
        "            return board[line[1]]\n",
        "\n",
        "    # tie\n",
        "    if(np.all(np.array(board) != 0)):\n",
        "        return 0.5\n",
        "\n",
        "    # game is not finished\n",
        "    return 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXrTo_vwVY2y"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paoZzPA1Vckz"
      },
      "source": [
        "### Agent ###\n",
        "\n",
        "def createPolicy():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer((9,)),\n",
        "        layers.Dense(10, activation = \"relu\"),\n",
        "        layers.Dense(10, activation = \"relu\"),\n",
        "        layers.Dense(9, activation = None)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9sczKIvAKML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0affe6c5-0073-4dda-bb49-e4f8e51e6a6b"
      },
      "source": [
        "policy = createPolicy()\n",
        "policy.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                100       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309\n",
            "Trainable params: 309\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dIASq1cVsb"
      },
      "source": [
        "def chooseAction(policy, board, epsilon = 0):\n",
        "    mask = [-np.inf if v != 0 else 0 for v in board]\n",
        "\n",
        "    if np.random.rand(1) > epsilon:\n",
        "        logits = policy.predict([board])\n",
        "\n",
        "        possibleLogits = tf.math.add(\n",
        "            logits, \n",
        "            mask\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        possibleLogits = [mask] + np.log(1/9)\n",
        "     \n",
        "    action = tf.random.categorical(possibleLogits, num_samples = 1)\n",
        "    action = action.numpy().flatten()[0]\n",
        "\n",
        "    return action"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUvb1yK-Ylt8"
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self): \n",
        "        self.clear()\n",
        "\n",
        "    def clear(self): \n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "        self.observations.append(new_observation)\n",
        "        self.actions.append(new_action)\n",
        "        self.rewards.append(new_reward)\n",
        "\n",
        "    # Helper function to combine a list of Memory objects into a single Memory.\n",
        "    # This will be very useful for batching.\n",
        "    def aggregate_memories(memories):\n",
        "        batch_memory = Memory()\n",
        "\n",
        "        for memory in memories:\n",
        "            for step in zip(memory.observations, memory.actions, memory.rewards):\n",
        "                batch_memory.add_to_memory(*step)\n",
        "\n",
        "        return batch_memory"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6al8fgdZZ7L"
      },
      "source": [
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= np.std(x)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma = 0.8): \n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    \n",
        "    R = 0\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        R = R * gamma + rewards[t]\n",
        "        discounted_rewards[t] = R\n",
        "        \n",
        "    return normalize(discounted_rewards)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1CT1UZeXfuz"
      },
      "source": [
        "def compute_loss(logits, actions, rewards):\n",
        "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, \n",
        "        labels=actions\n",
        "    )\n",
        "    # reinforce\n",
        "    loss = tf.reduce_mean(\n",
        "        neg_logprob * rewards\n",
        "    )\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhdMcQ-ImYEk"
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      logits = model(observations)\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2FOqw41nNuy"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWvrsjmYnO-l"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrpwyKjcnSeP"
      },
      "source": [
        "def plot(points, ylabel = \"\"):\n",
        "    plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
        "    plt.clf()\n",
        "    plt.plot(points)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xlabel(\"episode\")\n",
        "    plt.show()\n",
        "\n",
        "def smooth(points, alpha = 0.05):\n",
        "    smoothed = [points[0]]\n",
        "\n",
        "    for i in range(1, len(points)):\n",
        "        smoothed.append(points[i] * alpha + smoothed[i - 1] * (1 - alpha))\n",
        "\n",
        "    return smoothed"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxUpuDsbAuK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc92tQCwsH2u"
      },
      "source": [
        "winper_points = []\n",
        "reward_points = []\n",
        "hist_points = []\n",
        "\n",
        "episode = 0\n",
        "policy = createPolicy()\n",
        "\n",
        "memory = Memory()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82CNMdBia2js"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "opponent_policy = tf.keras.models.clone_model(policy)\n",
        "\n",
        "for episode in range(episode, 1000):\n",
        "    board = [0] * 9\n",
        "    memory.clear()\n",
        "    counter = 0\n",
        "    first_time = 1\n",
        "    win = 0\n",
        "\n",
        "    eps = 0.99 ** (episode / 2)\n",
        "\n",
        "    player_goes_first = bool(random.getrandbits(1))\n",
        "\n",
        "    while True:\n",
        "        if(player_goes_first and gameStatus(board) == 0):\n",
        "            action = chooseAction(policy, board, eps)\n",
        "\n",
        "        elif((not player_goes_first) and gameStatus(board) == 0):\n",
        "            if(first_time):\n",
        "                first_time -= 1\n",
        "                opponent_action = chooseAction(policy, (np.array(board) * -1).tolist(), eps)\n",
        "                board[opponent_action] = -1\n",
        "            action = chooseAction(policy, board, eps)\n",
        "\n",
        "        next_board = board\n",
        "        next_board[action] = 1\n",
        "\n",
        "        status = gameStatus(next_board)\n",
        "\n",
        "        if(status == 0): # game goes on\n",
        "            opponent_action = chooseAction(policy, (np.array(next_board) * -1).tolist())\n",
        "            next_board[opponent_action] = -1\n",
        "\n",
        "            status = gameStatus(next_board)\n",
        "    \n",
        "        reward = status\n",
        "\n",
        "        memory.add_to_memory(board, action, reward)\n",
        "\n",
        "        if(status != 0):\n",
        "            win += (status == 1)\n",
        "            counter += 1\n",
        "            next_board = [0] * 9\n",
        "            first_time = 1\n",
        "            player_goes_first = bool(random.getrandbits(1))\n",
        "        \n",
        "        if(status != 0 and counter >= 100):\n",
        "            loss = train_step(\n",
        "                policy, \n",
        "                optimizer, \n",
        "                observations = np.vstack(memory.observations),\n",
        "                actions = np.array(memory.actions),\n",
        "                discounted_rewards = discount_rewards(memory.rewards)\n",
        "            )\n",
        "\n",
        "            total_reward = sum(memory.rewards)\n",
        "            \n",
        "            winper_points.append(win / counter)\n",
        "            reward_points.append(total_reward)\n",
        "            hist_points.append(loss)\n",
        "\n",
        "            clear_output()\n",
        "            plot(smooth(hist_points, 0.01), \"loss\")\n",
        "            plot(smooth(winper_points, 0.01), \"win %\")\n",
        "            plot(smooth(reward_points, 0.01), \"reward\")\n",
        "\n",
        "            print(f\"{episode} loss: {loss}\")\n",
        "            print(f\"{episode} winPer: {win / counter}\")\n",
        "            print(f\"{episode} reward: {total_reward}\")\n",
        "\n",
        "            memory.clear()\n",
        "            board = [0] * 9\n",
        "            counter = 0\n",
        "            first_time = 1\n",
        "\n",
        "            break\n",
        "    \n",
        "        board = next_board"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioprskJo78Ba"
      },
      "source": [
        "# Web demo\n",
        "\n",
        "Play against the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGyNjHFhlJY"
      },
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "def symbol(x):\n",
        "    if x == 1:\n",
        "        return \"X\"\n",
        "    elif x == -1:\n",
        "        return \"O\"\n",
        "    else:\n",
        "        return \"?\"\n",
        "\n",
        "def printBoard(board):\n",
        "    clear_output()\n",
        "    cBoard = list(map(symbol, board))\n",
        "    for i in range(0, 3):\n",
        "        row = \"\"\n",
        "        for j in range(0, 3):\n",
        "            row += (cBoard[j + i * 3]) if (cBoard[j + i * 3] != \"?\") else str(j + i * 3 + 1)\n",
        "            if j != 2:\n",
        "                row += \" | \"\n",
        "        print(row)\n",
        "        if i != 2:\n",
        "            print(\"---------\")\n",
        "\n",
        "def play(policy):\n",
        "    player = (int(input(\"which player you want to be? (1 or 2) \")) + 1) % 2 \n",
        "\n",
        "    board = [0, 0, 0,\n",
        "             0, 0, 0,\n",
        "             0, 0, 0]\n",
        "\n",
        "    winner = 0\n",
        "\n",
        "    for i in range(9):\n",
        "        if (i % 2 == player):\n",
        "            printBoard(board)\n",
        "            action = int(input(\"what cell? \")) - 1\n",
        "        else:\n",
        "            action = chooseAction(policy, board.copy())\n",
        "\n",
        "        board[action] = 1\n",
        "\n",
        "        if(gameStatus(board) != 0):\n",
        "            winner = gameStatus(board)+1\n",
        "            break\n",
        "\n",
        "        board = (np.array(board) * -1).tolist()\n",
        "    \n",
        "    printBoard(board)\n",
        "    if(gameStatus(board) != 0):\n",
        "        print(\"\\nwinner is the\", \"humen\" if winner else \"ai\")\n",
        "    else:\n",
        "        print(\"\\nit's a tie!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuGYnTxMkMOd",
        "outputId": "4d16b8b7-81c6-44f9-edb2-41ac1c730f6e"
      },
      "source": [
        "play(policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X | X | O\n",
            "---------\n",
            "O | X | 6\n",
            "---------\n",
            "O | X | O\n",
            "\n",
            "winner is the humen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RyU3L043ogp"
      },
      "source": [
        "# Deploy to TF.js"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6h2d--F5EKx"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install tensorflowjs[wizard]\n",
        "!pip install -U ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBh6IN3m4O3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6300c8cb-d65c-4934-fd27-833fb801dbb8"
      },
      "source": [
        "policy.save(\"./model/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8mh__hd4wl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d02757-01ed-42ee-be49-8cbcc6477dd2"
      },
      "source": [
        "!tensorflowjs_converter --input_format=keras_saved_model /content/model /content/tfjs_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-26 13:28:26.764816: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    }
  ]
}