{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic Tac Toe DRL",
      "provenance": [],
      "authorship_tag": "ABX9TyMex6M62HYM+Kebgwcq0FWX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nomomon/drl-js/blob/main/tic-tac-toe/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBl0Klfol8d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTQEPi62q5i"
      },
      "source": [
        "# Game (Environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTm9-s892yVX"
      },
      "source": [
        "# 0 1 2\n",
        "# 3 4 5\n",
        "# 6 7 8\n",
        "\n",
        "def gameStatus(board, playerParity):\n",
        "    lines = [\n",
        "        [0, 1, 2],\n",
        "        [3, 4, 5],\n",
        "        [6, 7, 8],\n",
        "        [0, 3, 6],\n",
        "        [1, 4, 7],\n",
        "        [2, 5, 8],\n",
        "        [0, 4, 8],\n",
        "        [2, 4, 6],\n",
        "    ]\n",
        "\n",
        "    # there is a winner\n",
        "    for line in lines:\n",
        "        if (board[line[1]] == board[line[0]] and \n",
        "            board[line[1]] == board[line[2]] and \n",
        "            board[line[1]] != 0):\n",
        "            return ((np.sum(np.array(board) > 0) - playerParity) % 2 == 0) * 2 - 1\n",
        "\n",
        "    # tie\n",
        "    if(np.all(np.array(board) != 0)):\n",
        "        return 0.5\n",
        "\n",
        "    # game is not finished\n",
        "    return 0"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXrTo_vwVY2y"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paoZzPA1Vckz"
      },
      "source": [
        "### Agent ###\n",
        "\n",
        "def createPolicy():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer((9,)),\n",
        "        layers.Dense(20, activation = \"relu\"),\n",
        "        layers.Dropout(0.01),\n",
        "        layers.Dense(20, activation = \"relu\"),\n",
        "        layers.Dropout(0.01),\n",
        "        layers.Dense(9, activation = None)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dIASq1cVsb"
      },
      "source": [
        "def chooseAction(policy, board):\n",
        "    while True:\n",
        "        logits = policy.predict([board])\n",
        "        action = tf.random.categorical(logits, num_samples = 1)\n",
        "        action = action.numpy().flatten()[0]\n",
        "\n",
        "        # free cell\n",
        "        if(board[action] == 0):\n",
        "            break\n",
        "\n",
        "    return action"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUvb1yK-Ylt8"
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self): \n",
        "        self.clear()\n",
        "\n",
        "    def clear(self): \n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "        self.observations.append(new_observation)\n",
        "        self.actions.append(new_action)\n",
        "        self.rewards.append(new_reward)\n",
        "\n",
        "    # Helper function to combine a list of Memory objects into a single Memory.\n",
        "    # This will be very useful for batching.\n",
        "    def aggregate_memories(memories):\n",
        "        batch_memory = Memory()\n",
        "\n",
        "        for memory in memories:\n",
        "            for step in zip(memory.observations, memory.actions, memory.rewards):\n",
        "                batch_memory.add_to_memory(*step)\n",
        "\n",
        "        return batch_memory\n",
        "\n",
        "memory = Memory()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6al8fgdZZ7L"
      },
      "source": [
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= np.std(x)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma = 0.95): \n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    \n",
        "    R = 0\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        R = R * gamma + rewards[t]\n",
        "        discounted_rewards[t] = R\n",
        "        \n",
        "    return normalize(discounted_rewards)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1CT1UZeXfuz"
      },
      "source": [
        "def compute_loss(logits, actions, rewards):\n",
        "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, \n",
        "        labels=actions\n",
        "    )\n",
        "    loss = tf.reduce_mean(\n",
        "        neg_logprob * rewards\n",
        "    )\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhdMcQ-ImYEk"
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      logits = model(observations)\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2FOqw41nNuy"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWvrsjmYnO-l"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "points = np.array([])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrpwyKjcnSeP"
      },
      "source": [
        "def addToPlot(points, point):\n",
        "    i = points.shape[0] + 1\n",
        "    points = np.append(points, point)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(points, range(i))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxUpuDsbAuK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82CNMdBia2js",
        "outputId": "bef65367-9867-478e-a63e-8fb45aa03474"
      },
      "source": [
        "points = np.array([])\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "policy = createPolicy()\n",
        "\n",
        "for episode in range(500):\n",
        "    board = [0] * 9\n",
        "    memory.clear()\n",
        "\n",
        "    main_player = episode % 2\n",
        "\n",
        "    while True:\n",
        "        if(main_player == 1 and gameStatus(next_board, main_player) == 0):\n",
        "            opponent_action = chooseAction(policy, (np.array(board) * -1).tolist())\n",
        "            board[opponent_action] = -1\n",
        "\n",
        "        action = chooseAction(policy, board)\n",
        "\n",
        "        next_board = board\n",
        "        next_board[action] = 1\n",
        "        \n",
        "        if(main_player == 0 and gameStatus(next_board, main_player) == 0):\n",
        "            opponent_action = chooseAction(policy, (np.array(next_board) * -1).tolist())\n",
        "            next_board[opponent_action] = -1\n",
        "\n",
        "        status = gameStatus(next_board, main_player)\n",
        "\n",
        "        reward = status - 0.1\n",
        "        memory.add_to_memory(board, action, reward)\n",
        "\n",
        "        if(status != 0):\n",
        "            total_reward = sum(memory.rewards)\n",
        "            if(episode % 1 == 0):\n",
        "                print(f\"{episode} reward: {total_reward}\")\n",
        "\n",
        "            train_step(\n",
        "                policy, \n",
        "                optimizer, \n",
        "                observations = np.vstack(memory.observations),\n",
        "                actions = np.array(memory.actions),\n",
        "                discounted_rewards = discount_rewards(memory.rewards)\n",
        "            )\n",
        "\n",
        "            memory.clear()\n",
        "            board = [0] * 9\n",
        "\n",
        "            break\n",
        "\n",
        "        board = next_board"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 reward: 0.6\n",
            "1 reward: -1.4000000000000001\n",
            "2 reward: -1.5\n",
            "3 reward: 0.0\n",
            "4 reward: -1.5\n",
            "5 reward: -1.4000000000000001\n",
            "6 reward: 0.6\n",
            "7 reward: -1.4000000000000001\n",
            "8 reward: 0.0\n",
            "9 reward: -1.4000000000000001\n",
            "10 reward: 0.6\n",
            "11 reward: 0.5\n",
            "12 reward: 0.6\n",
            "13 reward: 0.5\n",
            "14 reward: -1.5\n",
            "15 reward: -1.4000000000000001\n",
            "16 reward: 0.6\n",
            "17 reward: 0.5\n",
            "18 reward: -1.3\n",
            "19 reward: 0.7\n",
            "20 reward: 0.6\n",
            "21 reward: -1.4000000000000001\n",
            "22 reward: -1.3\n",
            "23 reward: -1.4000000000000001\n",
            "24 reward: -1.5\n",
            "25 reward: 0.0\n",
            "26 reward: 0.6\n",
            "27 reward: -1.4000000000000001\n",
            "28 reward: 0.6\n",
            "29 reward: -1.4000000000000001\n",
            "30 reward: -1.3\n",
            "31 reward: -1.4000000000000001\n",
            "32 reward: 0.6\n",
            "33 reward: 0.5\n",
            "34 reward: 0.6\n",
            "35 reward: 0.5\n",
            "36 reward: -1.5\n",
            "37 reward: 0.5\n",
            "38 reward: 0.6\n",
            "39 reward: -1.4000000000000001\n",
            "40 reward: -1.5\n",
            "41 reward: -1.4000000000000001\n",
            "42 reward: 0.6\n",
            "43 reward: 0.5\n",
            "44 reward: 0.0\n",
            "45 reward: -1.4000000000000001\n",
            "46 reward: 0.0\n",
            "47 reward: 0.5\n",
            "48 reward: -1.5\n",
            "49 reward: -1.4000000000000001\n",
            "50 reward: -1.5\n",
            "51 reward: 0.7\n",
            "52 reward: 0.6\n",
            "53 reward: -1.4000000000000001\n",
            "54 reward: 0.6\n",
            "55 reward: 0.7\n",
            "56 reward: -1.3\n",
            "57 reward: -1.4000000000000001\n",
            "58 reward: -1.5\n",
            "59 reward: 0.5\n",
            "60 reward: 0.6\n",
            "61 reward: -1.4000000000000001\n",
            "62 reward: 0.6\n",
            "63 reward: -1.4000000000000001\n",
            "64 reward: 0.0\n",
            "65 reward: 0.5\n",
            "66 reward: -1.5\n",
            "67 reward: 0.5\n",
            "68 reward: -1.3\n",
            "69 reward: 0.5\n",
            "70 reward: 0.0\n",
            "71 reward: 0.0\n",
            "72 reward: 0.6\n",
            "73 reward: -1.4000000000000001\n",
            "74 reward: -1.5\n",
            "75 reward: 0.5\n",
            "76 reward: 0.6\n",
            "77 reward: -1.4000000000000001\n",
            "78 reward: 0.0\n",
            "79 reward: 0.5\n",
            "80 reward: 0.6\n",
            "81 reward: 0.5\n",
            "82 reward: 0.6\n",
            "83 reward: 0.5\n",
            "84 reward: -1.5\n",
            "85 reward: 0.0\n",
            "86 reward: -1.5\n",
            "87 reward: 0.5\n",
            "88 reward: -1.3\n",
            "89 reward: 0.5\n",
            "90 reward: 0.6\n",
            "91 reward: -1.4000000000000001\n",
            "92 reward: 0.6\n",
            "93 reward: -1.4000000000000001\n",
            "94 reward: 0.6\n",
            "95 reward: 0.5\n",
            "96 reward: -1.5\n",
            "97 reward: -1.4000000000000001\n",
            "98 reward: -1.5\n",
            "99 reward: 0.5\n",
            "100 reward: 0.6\n",
            "101 reward: -1.4000000000000001\n",
            "102 reward: -1.5\n",
            "103 reward: -1.4000000000000001\n",
            "104 reward: 0.0\n",
            "105 reward: 0.5\n",
            "106 reward: -1.3\n",
            "107 reward: 0.5\n",
            "108 reward: -1.5\n",
            "109 reward: -1.4000000000000001\n",
            "110 reward: 0.0\n",
            "111 reward: -1.4000000000000001\n",
            "112 reward: 0.0\n",
            "113 reward: 0.5\n",
            "114 reward: -1.5\n",
            "115 reward: -1.4000000000000001\n",
            "116 reward: 0.6\n",
            "117 reward: 0.0\n",
            "118 reward: 0.6\n",
            "119 reward: -1.4000000000000001\n",
            "120 reward: 0.6\n",
            "121 reward: -1.4000000000000001\n",
            "122 reward: 0.6\n",
            "123 reward: -1.4000000000000001\n",
            "124 reward: 0.0\n",
            "125 reward: 0.0\n",
            "126 reward: -1.3\n",
            "127 reward: -1.4000000000000001\n",
            "128 reward: 0.6\n",
            "129 reward: 0.0\n",
            "130 reward: -1.5\n",
            "131 reward: 0.5\n",
            "132 reward: -1.5\n",
            "133 reward: 0.5\n",
            "134 reward: -1.3\n",
            "135 reward: 0.5\n",
            "136 reward: -1.5\n",
            "137 reward: 0.5\n",
            "138 reward: 0.6\n",
            "139 reward: 0.0\n",
            "140 reward: 0.6\n",
            "141 reward: -1.4000000000000001\n",
            "142 reward: 0.0\n",
            "143 reward: 0.5\n",
            "144 reward: -1.5\n",
            "145 reward: 0.7\n",
            "146 reward: 0.6\n",
            "147 reward: -1.4000000000000001\n",
            "148 reward: -1.5\n",
            "149 reward: 0.5\n",
            "150 reward: 0.6\n",
            "151 reward: 0.5\n",
            "152 reward: 0.6\n",
            "153 reward: 0.5\n",
            "154 reward: 0.6\n",
            "155 reward: 0.7\n",
            "156 reward: 0.6\n",
            "157 reward: 0.5\n",
            "158 reward: -1.3\n",
            "159 reward: 0.5\n",
            "160 reward: 0.0\n",
            "161 reward: 0.5\n",
            "162 reward: 0.6\n",
            "163 reward: 0.5\n",
            "164 reward: -1.3\n",
            "165 reward: 0.0\n",
            "166 reward: -1.5\n",
            "167 reward: 0.5\n",
            "168 reward: 0.0\n",
            "169 reward: 0.7\n",
            "170 reward: 0.6\n",
            "171 reward: 0.7\n",
            "172 reward: 0.6\n",
            "173 reward: 0.7\n",
            "174 reward: -1.5\n",
            "175 reward: 0.5\n",
            "176 reward: -1.5\n",
            "177 reward: 0.5\n",
            "178 reward: -1.3\n",
            "179 reward: 0.5\n",
            "180 reward: -1.3\n",
            "181 reward: 0.7\n",
            "182 reward: 0.6\n",
            "183 reward: 0.5\n",
            "184 reward: -1.3\n",
            "185 reward: 0.0\n",
            "186 reward: 0.0\n",
            "187 reward: 0.5\n",
            "188 reward: -1.5\n",
            "189 reward: 0.5\n",
            "190 reward: -1.3\n",
            "191 reward: 0.7\n",
            "192 reward: 0.6\n",
            "193 reward: -1.4000000000000001\n",
            "194 reward: 0.6\n",
            "195 reward: 0.0\n",
            "196 reward: 0.0\n",
            "197 reward: 0.5\n",
            "198 reward: 0.6\n",
            "199 reward: 0.0\n",
            "200 reward: -1.5\n",
            "201 reward: 0.0\n",
            "202 reward: -1.5\n",
            "203 reward: 0.5\n",
            "204 reward: 0.6\n",
            "205 reward: 0.5\n",
            "206 reward: 0.0\n",
            "207 reward: 0.5\n",
            "208 reward: 0.6\n",
            "209 reward: -1.4000000000000001\n",
            "210 reward: 0.6\n",
            "211 reward: 0.0\n",
            "212 reward: -1.5\n",
            "213 reward: 0.5\n",
            "214 reward: -1.5\n",
            "215 reward: 0.5\n",
            "216 reward: 0.6\n",
            "217 reward: 0.5\n",
            "218 reward: 0.6\n",
            "219 reward: 0.7\n",
            "220 reward: -1.5\n",
            "221 reward: 0.5\n",
            "222 reward: -1.5\n",
            "223 reward: 0.5\n",
            "224 reward: 0.6\n",
            "225 reward: 0.5\n",
            "226 reward: -1.3\n",
            "227 reward: -1.4000000000000001\n",
            "228 reward: 0.0\n",
            "229 reward: 0.5\n",
            "230 reward: -1.3\n",
            "231 reward: -1.4000000000000001\n",
            "232 reward: -1.5\n",
            "233 reward: 0.0\n",
            "234 reward: 0.6\n",
            "235 reward: -1.4000000000000001\n",
            "236 reward: 0.6\n",
            "237 reward: 0.5\n",
            "238 reward: 0.6\n",
            "239 reward: 0.0\n",
            "240 reward: -1.5\n",
            "241 reward: -1.4000000000000001\n",
            "242 reward: 0.6\n",
            "243 reward: -1.4000000000000001\n",
            "244 reward: -1.5\n",
            "245 reward: 0.5\n",
            "246 reward: 0.0\n",
            "247 reward: 0.7\n",
            "248 reward: -1.3\n",
            "249 reward: 0.5\n",
            "250 reward: 0.6\n",
            "251 reward: 0.5\n",
            "252 reward: -1.5\n",
            "253 reward: -1.4000000000000001\n",
            "254 reward: -1.3\n",
            "255 reward: 0.0\n",
            "256 reward: -1.5\n",
            "257 reward: 0.7\n",
            "258 reward: 0.6\n",
            "259 reward: -1.4000000000000001\n",
            "260 reward: 0.6\n",
            "261 reward: 0.5\n",
            "262 reward: -1.3\n",
            "263 reward: 0.0\n",
            "264 reward: -1.3\n",
            "265 reward: -1.4000000000000001\n",
            "266 reward: 0.0\n",
            "267 reward: 0.5\n",
            "268 reward: 0.6\n",
            "269 reward: -1.4000000000000001\n",
            "270 reward: -1.3\n",
            "271 reward: -1.4000000000000001\n",
            "272 reward: -1.3\n",
            "273 reward: 0.5\n",
            "274 reward: 0.6\n",
            "275 reward: 0.5\n",
            "276 reward: 0.6\n",
            "277 reward: 0.5\n",
            "278 reward: 0.6\n",
            "279 reward: -1.4000000000000001\n",
            "280 reward: -1.3\n",
            "281 reward: 0.7\n",
            "282 reward: -1.3\n",
            "283 reward: 0.5\n",
            "284 reward: 0.0\n",
            "285 reward: 0.0\n",
            "286 reward: 0.6\n",
            "287 reward: -1.4000000000000001\n",
            "288 reward: 0.6\n",
            "289 reward: -1.4000000000000001\n",
            "290 reward: -1.5\n",
            "291 reward: -1.4000000000000001\n",
            "292 reward: 0.6\n",
            "293 reward: 0.5\n",
            "294 reward: 0.6\n",
            "295 reward: 0.5\n",
            "296 reward: -1.3\n",
            "297 reward: -1.4000000000000001\n",
            "298 reward: -1.5\n",
            "299 reward: 0.5\n",
            "300 reward: -1.5\n",
            "301 reward: -1.4000000000000001\n",
            "302 reward: -1.3\n",
            "303 reward: -1.4000000000000001\n",
            "304 reward: -1.3\n",
            "305 reward: 0.5\n",
            "306 reward: -1.5\n",
            "307 reward: -1.4000000000000001\n",
            "308 reward: -1.3\n",
            "309 reward: 0.5\n",
            "310 reward: -1.5\n",
            "311 reward: 0.0\n",
            "312 reward: -1.5\n",
            "313 reward: 0.5\n",
            "314 reward: 0.6\n",
            "315 reward: -1.4000000000000001\n",
            "316 reward: 0.6\n",
            "317 reward: 0.5\n",
            "318 reward: 0.6\n",
            "319 reward: 0.5\n",
            "320 reward: 0.6\n",
            "321 reward: 0.0\n",
            "322 reward: -1.5\n",
            "323 reward: 0.5\n",
            "324 reward: 0.6\n",
            "325 reward: 0.0\n",
            "326 reward: 0.6\n",
            "327 reward: 0.5\n",
            "328 reward: -1.5\n",
            "329 reward: -1.4000000000000001\n",
            "330 reward: 0.0\n",
            "331 reward: 0.5\n",
            "332 reward: -1.5\n",
            "333 reward: 0.0\n",
            "334 reward: 0.6\n",
            "335 reward: -1.4000000000000001\n",
            "336 reward: -1.5\n",
            "337 reward: 0.5\n",
            "338 reward: -1.3\n",
            "339 reward: 0.5\n",
            "340 reward: 0.6\n",
            "341 reward: 0.7\n",
            "342 reward: -1.3\n",
            "343 reward: 0.5\n",
            "344 reward: 0.6\n",
            "345 reward: 0.5\n",
            "346 reward: 0.6\n",
            "347 reward: 0.7\n",
            "348 reward: 0.6\n",
            "349 reward: 0.7\n",
            "350 reward: 0.6\n",
            "351 reward: -1.4000000000000001\n",
            "352 reward: 0.0\n",
            "353 reward: 0.0\n",
            "354 reward: -1.5\n",
            "355 reward: 0.0\n",
            "356 reward: -1.5\n",
            "357 reward: 0.5\n",
            "358 reward: -1.3\n",
            "359 reward: 0.5\n",
            "360 reward: 0.6\n",
            "361 reward: 0.5\n",
            "362 reward: -1.5\n",
            "363 reward: 0.5\n",
            "364 reward: -1.5\n",
            "365 reward: 0.5\n",
            "366 reward: 0.0\n",
            "367 reward: 0.5\n",
            "368 reward: 0.6\n",
            "369 reward: -1.4000000000000001\n",
            "370 reward: 0.6\n",
            "371 reward: 0.5\n",
            "372 reward: 0.6\n",
            "373 reward: 0.5\n",
            "374 reward: 0.6\n",
            "375 reward: 0.5\n",
            "376 reward: -1.3\n",
            "377 reward: 0.0\n",
            "378 reward: 0.6\n",
            "379 reward: 0.5\n",
            "380 reward: 0.6\n",
            "381 reward: 0.7\n",
            "382 reward: -1.3\n",
            "383 reward: -1.4000000000000001\n",
            "384 reward: -1.3\n",
            "385 reward: -1.4000000000000001\n",
            "386 reward: -1.5\n",
            "387 reward: -1.4000000000000001\n",
            "388 reward: 0.6\n",
            "389 reward: -1.4000000000000001\n",
            "390 reward: 0.0\n",
            "391 reward: -1.4000000000000001\n",
            "392 reward: -1.5\n",
            "393 reward: 0.5\n",
            "394 reward: -1.3\n",
            "395 reward: 0.5\n",
            "396 reward: 0.6\n",
            "397 reward: 0.5\n",
            "398 reward: -1.5\n",
            "399 reward: 0.5\n",
            "400 reward: -1.5\n",
            "401 reward: -1.4000000000000001\n",
            "402 reward: -1.3\n",
            "403 reward: 0.5\n",
            "404 reward: 0.6\n",
            "405 reward: 0.7\n",
            "406 reward: 0.6\n",
            "407 reward: 0.5\n",
            "408 reward: -1.5\n",
            "409 reward: 0.5\n",
            "410 reward: 0.0\n",
            "411 reward: -1.4000000000000001\n",
            "412 reward: 0.6\n",
            "413 reward: 0.5\n",
            "414 reward: -1.5\n",
            "415 reward: -1.4000000000000001\n",
            "416 reward: 0.0\n",
            "417 reward: 0.5\n",
            "418 reward: 0.6\n",
            "419 reward: 0.5\n",
            "420 reward: -1.5\n",
            "421 reward: -1.4000000000000001\n",
            "422 reward: 0.6\n",
            "423 reward: -1.4000000000000001\n",
            "424 reward: -1.5\n",
            "425 reward: -1.4000000000000001\n",
            "426 reward: 0.6\n",
            "427 reward: -1.4000000000000001\n",
            "428 reward: 0.6\n",
            "429 reward: -1.4000000000000001\n",
            "430 reward: 0.0\n",
            "431 reward: 0.0\n",
            "432 reward: -1.3\n",
            "433 reward: 0.5\n",
            "434 reward: 0.0\n",
            "435 reward: 0.5\n",
            "436 reward: -1.3\n",
            "437 reward: -1.4000000000000001\n",
            "438 reward: -1.5\n",
            "439 reward: -1.4000000000000001\n",
            "440 reward: -1.3\n",
            "441 reward: -1.4000000000000001\n",
            "442 reward: 0.6\n",
            "443 reward: 0.5\n",
            "444 reward: 0.6\n",
            "445 reward: 0.5\n",
            "446 reward: -1.5\n",
            "447 reward: 0.7\n",
            "448 reward: 0.6\n",
            "449 reward: 0.5\n",
            "450 reward: -1.3\n",
            "451 reward: 0.0\n",
            "452 reward: 0.6\n",
            "453 reward: 0.5\n",
            "454 reward: 0.6\n",
            "455 reward: 0.7\n",
            "456 reward: -1.5\n",
            "457 reward: 0.5\n",
            "458 reward: -1.5\n",
            "459 reward: 0.5\n",
            "460 reward: -1.3\n",
            "461 reward: -1.4000000000000001\n",
            "462 reward: 0.6\n",
            "463 reward: 0.5\n",
            "464 reward: 0.6\n",
            "465 reward: 0.5\n",
            "466 reward: 0.0\n",
            "467 reward: -1.4000000000000001\n",
            "468 reward: -1.5\n",
            "469 reward: 0.5\n",
            "470 reward: 0.0\n",
            "471 reward: 0.7\n",
            "472 reward: 0.6\n",
            "473 reward: 0.5\n",
            "474 reward: -1.3\n",
            "475 reward: -1.4000000000000001\n",
            "476 reward: 0.6\n",
            "477 reward: 0.5\n",
            "478 reward: -1.5\n",
            "479 reward: -1.4000000000000001\n",
            "480 reward: 0.6\n",
            "481 reward: 0.0\n",
            "482 reward: 0.0\n",
            "483 reward: -1.4000000000000001\n",
            "484 reward: 0.6\n",
            "485 reward: -1.4000000000000001\n",
            "486 reward: -1.3\n",
            "487 reward: -1.4000000000000001\n",
            "488 reward: -1.5\n",
            "489 reward: 0.5\n",
            "490 reward: 0.6\n",
            "491 reward: -1.4000000000000001\n",
            "492 reward: 0.6\n",
            "493 reward: -1.4000000000000001\n",
            "494 reward: -1.5\n",
            "495 reward: 0.5\n",
            "496 reward: 0.0\n",
            "497 reward: 0.5\n",
            "498 reward: 0.6\n",
            "499 reward: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioprskJo78Ba"
      },
      "source": [
        "# Web demo\n",
        "\n",
        "Play against the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGyNjHFhlJY"
      },
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "def symbol(x):\n",
        "    if x == 1:\n",
        "        return \"X\"\n",
        "    elif x == -1:\n",
        "        return \"O\"\n",
        "    else:\n",
        "        return \"?\"\n",
        "\n",
        "def printBoard(board):\n",
        "    clear_output()\n",
        "    cBoard = list(map(symbol, board))\n",
        "    for i in range(0, 3):\n",
        "        row = \"\"\n",
        "        for j in range(0, 3):\n",
        "            row += (cBoard[j + i * 3]) if (cBoard[j + i * 3] != \"?\") else str(j + i * 3 + 1)\n",
        "            if j != 2:\n",
        "                row += \" | \"\n",
        "        print(row)\n",
        "        if i != 2:\n",
        "            print(\"---------\")\n",
        "\n",
        "def play(policy):\n",
        "    player = (int(input(\"which player you want to be? (1 or 2) \")) + 1) % 2 \n",
        "\n",
        "    board = [0, 0, 0,\n",
        "             0, 0, 0,\n",
        "             0, 0, 0]\n",
        "\n",
        "    winner = 0\n",
        "\n",
        "    for i in range(9):\n",
        "        if (i % 2 == player):\n",
        "            printBoard(board)\n",
        "            action = int(input(\"what cell? \")) - 1\n",
        "        else:\n",
        "            action = chooseAction(policy, board.copy())\n",
        "\n",
        "        board[action] = 1\n",
        "\n",
        "        if(gameStatus(board, player) != 0):\n",
        "            winner = gameStatus(board, player)\n",
        "            break\n",
        "\n",
        "        board = (np.array(board) * -1).tolist()\n",
        "    \n",
        "    printBoard(board)\n",
        "    if(gameStatus(board) != 0):\n",
        "        print(\"\\nwinner is the\", \"humen\" if winner else \"ai\")\n",
        "    else:\n",
        "        print(\"\\nit's a tie!\")"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "NuGYnTxMkMOd",
        "outputId": "9c8a39fc-28b0-4c63-e9da-ddfda401e199"
      },
      "source": [
        "play(policy)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X | 2 | O\n",
            "---------\n",
            "4 | X | O\n",
            "---------\n",
            "7 | O | X\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-a5184b472d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-176-2deaa108a071>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(policy)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprintBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgameStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nwinner is the\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"humen\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwinner\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"ai\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: gameStatus() missing 1 required positional argument: 'playerParity'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RyU3L043ogp"
      },
      "source": [
        "# Deploy to TF.js"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6h2d--F5EKx"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install tensorflowjs[wizard]\n",
        "!pip install -U ipython"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBh6IN3m4O3l",
        "outputId": "3f01d5a7-e0c0-4ff9-84e0-1283d5a29c9b"
      },
      "source": [
        "policy.save(\"./model/\")"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8mh__hd4wl_",
        "outputId": "d3784188-bcdb-4b29-ed7f-7d591f706b19"
      },
      "source": [
        "!tensorflowjs_converter --input_format=keras_saved_model /content/model /content/tfjs_model"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-12 22:01:36.565801: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    }
  ]
}