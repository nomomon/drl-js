{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic Tac Toe DRL",
      "provenance": [],
      "authorship_tag": "ABX9TyPHztFfs/9GYsstONP40lZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nomomon/drl-js/blob/main/tic-tac-toe/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBl0Klfol8d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTQEPi62q5i"
      },
      "source": [
        "# Game (Environment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTm9-s892yVX"
      },
      "source": [
        "# 0 1 2\n",
        "# 3 4 5\n",
        "# 6 7 8\n",
        "\n",
        "def gameStatus(board):\n",
        "    lines = [\n",
        "        [0, 1, 2],\n",
        "        [3, 4, 5],\n",
        "        [6, 7, 8],\n",
        "        [0, 3, 6],\n",
        "        [1, 4, 7],\n",
        "        [2, 5, 8],\n",
        "        [0, 4, 8],\n",
        "        [2, 4, 6],\n",
        "    ]\n",
        "\n",
        "    # there is a winner\n",
        "    for line in lines:\n",
        "        if (board[line[1]] == board[line[0]] and \n",
        "            board[line[1]] == board[line[2]] and \n",
        "            board[line[1]] != 0):\n",
        "            return board[line[1]]\n",
        "\n",
        "    # tie\n",
        "    if(np.all(np.array(board) != 0)):\n",
        "        return 0.5\n",
        "\n",
        "    # game is not finished\n",
        "    return 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXrTo_vwVY2y"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paoZzPA1Vckz"
      },
      "source": [
        "### Agent ###\n",
        "\n",
        "def createPolicy():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer((9,)),\n",
        "        layers.Reshape((3, 3, 1,)),\n",
        "        layers.Conv2D(8, (3, 3)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(20, activation = \"relu\"),\n",
        "        layers.Dense(20, activation = \"relu\"),\n",
        "        layers.Dense(9, activation = None)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dIASq1cVsb"
      },
      "source": [
        "def chooseAction(policy, board):\n",
        "    while True:\n",
        "        logits = policy.predict([board])\n",
        "\n",
        "        possible = (board == 0) + 0\n",
        "        possibleLogits = tf.math.multiply(logits, possible)\n",
        "\n",
        "        action = tf.random.categorical(possibleLogits, num_samples = 1)\n",
        "        action = action.numpy().flatten()[0]\n",
        "\n",
        "        # free cell\n",
        "        if(board[action] == 0):\n",
        "            break\n",
        "\n",
        "    return action"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUvb1yK-Ylt8"
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self): \n",
        "        self.clear()\n",
        "\n",
        "    def clear(self): \n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "        self.observations.append(new_observation)\n",
        "        self.actions.append(new_action)\n",
        "        self.rewards.append(new_reward)\n",
        "\n",
        "    # Helper function to combine a list of Memory objects into a single Memory.\n",
        "    # This will be very useful for batching.\n",
        "    def aggregate_memories(memories):\n",
        "        batch_memory = Memory()\n",
        "\n",
        "        for memory in memories:\n",
        "            for step in zip(memory.observations, memory.actions, memory.rewards):\n",
        "                batch_memory.add_to_memory(*step)\n",
        "\n",
        "        return batch_memory\n",
        "\n",
        "memory = Memory()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6al8fgdZZ7L"
      },
      "source": [
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= np.std(x)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma = 0.95): \n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    \n",
        "    R = 0\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        R = R * gamma + rewards[t]\n",
        "        discounted_rewards[t] = R\n",
        "        \n",
        "    return normalize(discounted_rewards)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1CT1UZeXfuz"
      },
      "source": [
        "def compute_loss(logits, actions, rewards):\n",
        "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, \n",
        "        labels=actions\n",
        "    )\n",
        "    loss = tf.reduce_mean(\n",
        "        neg_logprob * rewards\n",
        "    )\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhdMcQ-ImYEk"
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      logits = model(observations)\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2FOqw41nNuy"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWvrsjmYnO-l"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrpwyKjcnSeP"
      },
      "source": [
        "def plot(points):\n",
        "    clear_output()\n",
        "    plt.clf()\n",
        "    plt.plot(points)\n",
        "    plt.show()\n",
        "\n",
        "def smooth(points, alpha = 0.6):\n",
        "    smoothed = [points[0]]\n",
        "\n",
        "    for i in range(1, len(points)):\n",
        "        smoothed.append(points[i] * alpha + smoothed[i - 1] * (1 - alpha))\n",
        "\n",
        "    return smoothed"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxUpuDsbAuK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc92tQCwsH2u"
      },
      "source": [
        "points = []\n",
        "episode = 0\n",
        "policy = createPolicy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82CNMdBia2js"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "for episode in range(episode, 500):\n",
        "    board = [0] * 9\n",
        "    memory.clear()\n",
        "    counter = 0\n",
        "    first_time = 1\n",
        "    step = 0\n",
        "    win = 0\n",
        "\n",
        "    player_goes_first = bool(random.getrandbits(1))\n",
        "\n",
        "    while True:\n",
        "        if(player_goes_first and gameStatus(board) == 0):\n",
        "            action = chooseAction(policy, board)\n",
        "\n",
        "            next_board = board\n",
        "            next_board[action] = 1\n",
        "\n",
        "            status = gameStatus(next_board)\n",
        "\n",
        "            if(status == 0): # game goes on\n",
        "                opponent_action = chooseAction(policy, (np.array(next_board) * -1).tolist())\n",
        "                next_board[opponent_action] = -1\n",
        "\n",
        "                status = gameStatus(next_board)\n",
        "        \n",
        "            reward = status - 0.01 * step\n",
        "            memory.add_to_memory(board, action, reward)\n",
        "\n",
        "            if(status != 0):\n",
        "                win += (status == 1)\n",
        "                counter += 1\n",
        "                next_board = [0] * 9\n",
        "                first_time = 1\n",
        "                step = 0\n",
        "                player_goes_first = bool(random.getrandbits(1))\n",
        "            \n",
        "            if(status != 0 and counter >= 20):\n",
        "                total_reward = sum(memory.rewards)\n",
        "                points.append(win / counter)\n",
        "                plot(smooth(points))\n",
        "                print(f\"{episode} reward: {total_reward}\")\n",
        "                print(f\"{episode} winRat: {win / counter}\")\n",
        "\n",
        "                train_step(\n",
        "                    policy, \n",
        "                    optimizer, \n",
        "                    observations = np.vstack(memory.observations),\n",
        "                    actions = np.array(memory.actions),\n",
        "                    discounted_rewards = discount_rewards(memory.rewards)\n",
        "                )\n",
        "\n",
        "                memory.clear()\n",
        "                board = [0] * 9\n",
        "                counter = 0\n",
        "                first_time = 1\n",
        "                step = 0\n",
        "\n",
        "                break\n",
        "        \n",
        "            board = next_board\n",
        "\n",
        "        elif((not player_goes_first) and gameStatus(board) == 0):\n",
        "            if(first_time):\n",
        "                first_time -= 1\n",
        "                opponent_action = chooseAction(policy, (np.array(board) * -1).tolist())\n",
        "                board[opponent_action] = -1\n",
        "\n",
        "            action = chooseAction(policy, board)\n",
        "\n",
        "            next_board = board\n",
        "            next_board[action] = 1\n",
        "\n",
        "            status = gameStatus(next_board)\n",
        "\n",
        "            if(status == 0): # game goes on\n",
        "                opponent_action = chooseAction(policy, (np.array(next_board) * -1).tolist())\n",
        "                next_board[opponent_action] = -1\n",
        "\n",
        "                status = gameStatus(next_board)\n",
        "        \n",
        "            reward = status - 0.01 * step\n",
        "            memory.add_to_memory(board, action, reward)\n",
        "\n",
        "            if(status != 0):\n",
        "                win += (status == 1)\n",
        "                counter += 1\n",
        "                next_board = [0] * 9\n",
        "                first_time = 1\n",
        "                step = 0\n",
        "                player_goes_first = bool(random.getrandbits(1))\n",
        "            \n",
        "            if(status != 0 and counter >= 20):\n",
        "                total_reward = sum(memory.rewards)\n",
        "                points.append(win / counter)\n",
        "                plot(smooth(points))\n",
        "                print(f\"{episode} reward: {total_reward}\")\n",
        "                print(f\"{episode} winRat: {win / counter}\")\n",
        "\n",
        "                train_step(\n",
        "                    policy, \n",
        "                    optimizer, \n",
        "                    observations = np.vstack(memory.observations),\n",
        "                    actions = np.array(memory.actions),\n",
        "                    discounted_rewards = discount_rewards(memory.rewards)\n",
        "                )\n",
        "\n",
        "                memory.clear()\n",
        "                board = [0] * 9\n",
        "                counter = 0\n",
        "                first_time = 1\n",
        "                step = 0\n",
        "\n",
        "                break\n",
        "        \n",
        "            board = next_board\n",
        "        \n",
        "        else:\n",
        "            print(player_goes_first, gameStatus(board))\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioprskJo78Ba"
      },
      "source": [
        "# Web demo\n",
        "\n",
        "Play against the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGyNjHFhlJY"
      },
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "def symbol(x):\n",
        "    if x == 1:\n",
        "        return \"X\"\n",
        "    elif x == -1:\n",
        "        return \"O\"\n",
        "    else:\n",
        "        return \"?\"\n",
        "\n",
        "def printBoard(board):\n",
        "    clear_output()\n",
        "    cBoard = list(map(symbol, board))\n",
        "    for i in range(0, 3):\n",
        "        row = \"\"\n",
        "        for j in range(0, 3):\n",
        "            row += (cBoard[j + i * 3]) if (cBoard[j + i * 3] != \"?\") else str(j + i * 3 + 1)\n",
        "            if j != 2:\n",
        "                row += \" | \"\n",
        "        print(row)\n",
        "        if i != 2:\n",
        "            print(\"---------\")\n",
        "\n",
        "def play(policy):\n",
        "    player = (int(input(\"which player you want to be? (1 or 2) \")) + 1) % 2 \n",
        "\n",
        "    board = [0, 0, 0,\n",
        "             0, 0, 0,\n",
        "             0, 0, 0]\n",
        "\n",
        "    winner = 0\n",
        "\n",
        "    for i in range(9):\n",
        "        if (i % 2 == player):\n",
        "            printBoard(board)\n",
        "            action = int(input(\"what cell? \")) - 1\n",
        "        else:\n",
        "            action = chooseAction(policy, board.copy())\n",
        "\n",
        "        board[action] = 1\n",
        "\n",
        "        if(gameStatus(board) != 0):\n",
        "            winner = gameStatus(board)+1\n",
        "            break\n",
        "\n",
        "        board = (np.array(board) * -1).tolist()\n",
        "    \n",
        "    printBoard(board)\n",
        "    if(gameStatus(board) != 0):\n",
        "        print(\"\\nwinner is the\", \"humen\" if winner else \"ai\")\n",
        "    else:\n",
        "        print(\"\\nit's a tie!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuGYnTxMkMOd",
        "outputId": "6a84203f-e49e-4233-ac49-c229bf30bc50"
      },
      "source": [
        "play(policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 | X | O\n",
            "---------\n",
            "O | X | 6\n",
            "---------\n",
            "7 | X | O\n",
            "\n",
            "winner is the humen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RyU3L043ogp"
      },
      "source": [
        "# Deploy to TF.js"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6h2d--F5EKx"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install tensorflowjs[wizard]\n",
        "!pip install -U ipython"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBh6IN3m4O3l",
        "outputId": "b3b4b9ed-a6c9-4513-f28e-408aa938e1d3"
      },
      "source": [
        "policy.save(\"./model/\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8mh__hd4wl_",
        "outputId": "e675aa56-6572-4861-810a-2fece7d9fc64"
      },
      "source": [
        "!tensorflowjs_converter --input_format=keras_saved_model /content/model /content/tfjs_model"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-16 12:59:12.108668: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    }
  ]
}