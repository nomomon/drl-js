{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL â€“ Snake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQUGyjZ35h1OjHhovRefQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nomomon/drl-js/blob/main/snake/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irQqxiNCcvPp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub5ddJefPW-r"
      },
      "source": [
        "class snakeEnvironment:\n",
        "    def __init__(self, boardWidth = 20, boardHeight = 20):\n",
        "        self.width = boardWidth\n",
        "        self.height = boardHeight\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.stepsLeft = self.width * self.height\n",
        "\n",
        "        self.board = np.zeros((self.width, self.height))\n",
        "        self.snake = []\n",
        "        self.snakeDirection = 0\n",
        "        self.dead = 0\n",
        "\n",
        "        self.initBoard()\n",
        "\n",
        "    def initBoard(self):\n",
        "        # place apple\n",
        "        empty = np.stack(np.where(self.board == 0))\n",
        "        emptyPointIndex = np.random.randint(empty.shape[0], size = 1)[0]\n",
        "        emptyPoint = empty[emptyPointIndex]\n",
        "        self.apple = emptyPoint\n",
        "        self.board[emptyPoint[0]][emptyPoint[1]] = 2\n",
        "        \n",
        "        # place snake\n",
        "        empty = np.stack(np.where(self.board == 0))\n",
        "        emptyPointIndex = np.random.randint(empty.shape[0], size = 1)[0]\n",
        "        emptyPoint = empty[emptyPointIndex]\n",
        "        self.board[emptyPoint[0]][emptyPoint[1]] = 1\n",
        "\n",
        "        self.snakeHead = emptyPoint\n",
        "        self.snake = [emptyPoint]\n",
        "\n",
        "    def getObservations(self):\n",
        "\n",
        "        apple_x, apple_y = self.apple[0], self.apple[1]\n",
        "        snake_x, snake_y = self.snakeHead[0], self.snakeHead[1]\n",
        "\n",
        "        danger_straight = 0\n",
        "        danger_right = 0\n",
        "        danger_left =  0\n",
        "\n",
        "        for action, danger in enumerate([danger_left, danger_straight, danger_right]):\n",
        "            newSnakeDirection = (4 + self.snakeDirection + self.getActions()[action]) % 4\n",
        "\n",
        "            if(newSnakeDirection == 0):\n",
        "                nextState = (self.snake[0][0] + 1, self.snake[0][1])    # up\n",
        "            elif(newSnakeDirection == 1):\n",
        "                nextState = (self.snake[0][0], self.snake[0][1] + 1)    # right\n",
        "            elif(newSnakeDirection == 2):\n",
        "                nextState = (self.snake[0][0] - 1, self.snake[0][1])    # down\n",
        "            elif(newSnakeDirection == 3):\n",
        "                nextState = (self.snake[0][0], self.snake[0][1] - 1)    # left\n",
        "\n",
        "            if(self.board[nextState[0]][nextState[1]] not in [0, 2]):\n",
        "                danger = 1\n",
        "\n",
        "        moving_up =    (self.snakeDirection == 0) + 0\n",
        "        moving_right = (self.snakeDirection == 1) + 0\n",
        "        moving_left =  (self.snakeDirection == 3) + 0\n",
        "        moving_down =  (self.snakeDirection == 3) + 0\n",
        "        food_left =    (snake_x > apple_x) + 0\n",
        "        food_right =   (snake_x < apple_x) + 0\n",
        "        food_up =      (snake_y > apple_y) + 0\n",
        "        food_down =    (snake_y < apple_y) + 0\n",
        "\n",
        "        return [danger_straight, danger_right, danger_left, moving_left, moving_right, moving_up, moving_down, food_left, food_right, food_up, food_down]\n",
        "    \n",
        "    def getActions(self):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def isDone(self) -> bool:\n",
        "        return (self.stepsLeft == 0) or self.dead\n",
        "\n",
        "    def executeAction(self, action):\n",
        "        if(self.isDone()):\n",
        "            raise Exception(\"Game is over, however tried to execute an action\")\n",
        "        \n",
        "        self.stepsLeft -= 1\n",
        "\n",
        "        self.rewardForAction = 0\n",
        "        self.dead = 0\n",
        "\n",
        "        # -1 - turn left\n",
        "        #  0 - continue same direction\n",
        "        #  1 - turn right\n",
        "\n",
        "        # snake[0]  - head\n",
        "        # snake[-1] - tail\n",
        "\n",
        "        newSnakeDirection = (4 + self.snakeDirection + self.getActions()[action]) % 4\n",
        "\n",
        "        if(newSnakeDirection == 0):\n",
        "            nextState = (self.snake[0][0] + 1, self.snake[0][1])    # up\n",
        "        elif(newSnakeDirection == 1):\n",
        "            nextState = (self.snake[0][0], self.snake[0][1] + 1)    # right\n",
        "        elif(newSnakeDirection == 2):\n",
        "            nextState = (self.snake[0][0] - 1, self.snake[0][1])    # down\n",
        "        elif(newSnakeDirection == 3):\n",
        "            nextState = (self.snake[0][0], self.snake[0][1] - 1)    # left\n",
        "        \n",
        "        self.snakeDirection = newSnakeDirection\n",
        "\n",
        "        # hit a wall or it self\n",
        "        if(self.board[nextState[0]][nextState[1]] == -1 or \n",
        "           self.board[nextState[0]][nextState[1]] == 1):\n",
        "\n",
        "            self.dead = 1\n",
        "            self.rewardForAction = -1\n",
        "        \n",
        "        # ate the apple\n",
        "        if(self.board[nextState[0]][nextState[1]] == 2):\n",
        "            # update the snake\n",
        "            self.snakeHead = nextState\n",
        "            self.board[nextState[0]][nextState[1]] = 1\n",
        "            self.snake = np.concatenate(nextState, self.snake)\n",
        "\n",
        "            # new apple\n",
        "            empty = np.stack(np.where(self.board == 0))\n",
        "            newPointIndex = np.random.randint(empty.shape[0], size = 1)[0]\n",
        "            newPoint = empty[newPointIndex]\n",
        "            self.apple = newPoint\n",
        "            self.board[newPoint[0]][newPoint[1]] = 2\n",
        "\n",
        "            self.rewardForAction = 1\n",
        "        \n",
        "        # nothing happened\n",
        "        if(self.board[nextState[0]][nextState[1]] == 0):\n",
        "            self.snakeHead = nextState\n",
        "            self.board[nextState[0]][nextState[1]] = 1\n",
        "            self.snake = np.concatenate(nextState, self.snake)[:-1]\n",
        "\n",
        "    def step(self, action):\n",
        "        self.executeAction(action)\n",
        "\n",
        "        return self.getObservations(), self.rewardForAction, self.isDone()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPeWcwVzThrT"
      },
      "source": [
        "env = snakeEnvironment()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwmv3F-AT9O3",
        "outputId": "d25b8615-de59-4bc4-c9a5-0961420dec70"
      },
      "source": [
        "n_observations = len(env.getObservations())\n",
        "print(f\"Observation space is size {n_observations}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space is size 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcpdcOUoTq9G",
        "outputId": "84c7fc20-aeef-45f1-87a4-6229e75a54e5"
      },
      "source": [
        "n_actions = len(env.getActions())\n",
        "print(f\"Action space is size {n_actions}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space is size 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riKhbNldTULq"
      },
      "source": [
        "# model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWYFCwCSaJ9"
      },
      "source": [
        "### Agent ###\n",
        "\n",
        "def createPolicy():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer((n_observations,)),\n",
        "        tf.keras.layers.Dense(100, activation = \"relu\"),\n",
        "        tf.keras.layers.Dense(100, activation = \"relu\"),\n",
        "        tf.keras.layers.Dense(n_actions, activation = None)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ-z_dxXU0fW"
      },
      "source": [
        "def choose_action(model, observation, single=True):\n",
        "    observation = np.expand_dims(observation, axis=0) if single else observation\n",
        "    \n",
        "    logits = model.predict(observation)\n",
        "    action = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "    action = action.numpy().flatten()\n",
        "\n",
        "    return action[0] if single else action"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAMyBMylVUJ5"
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self): \n",
        "        self.clear()\n",
        "\n",
        "    def clear(self): \n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "        self.observations.append(new_observation)\n",
        "        self.actions.append(new_action)\n",
        "        self.rewards.append(new_reward)\n",
        "\n",
        "    def aggregate_memories(memories):\n",
        "        batch_memory = Memory()\n",
        "        \n",
        "        for memory in memories:\n",
        "            for step in zip(memory.observations, memory.actions, memory.rewards):\n",
        "                batch_memory.add_to_memory(*step)\n",
        "\n",
        "        return batch_memory\n",
        "\n",
        "memory = Memory()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snv9DhRmVg1V"
      },
      "source": [
        "### Reward function ###\n",
        "\n",
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= np.std(x)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    R = 0\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        R = R * gamma + rewards[t]\n",
        "        discounted_rewards[t] = R\n",
        "        \n",
        "    return normalize(discounted_rewards)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Ym6aplVntQ"
      },
      "source": [
        "### Loss function ###\n",
        "\n",
        "def compute_loss(logits, actions, rewards):\n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=logits, labels=actions)\n",
        "  loss = tf.reduce_mean( neg_logprob * rewards )\n",
        "  return loss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pNOBrKtVuY4"
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(observations)\n",
        "\n",
        "        loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "Ed_jledUV3sq",
        "outputId": "2c0e17a6-502e-41de-b721-0d0f5ae1260d"
      },
      "source": [
        "### Cartpole training! ###\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "snake_model = createPolicy()\n",
        "\n",
        "for i_episode in range(500):\n",
        "\n",
        "    env.reset()\n",
        "    observation = env.getObservations()\n",
        "    memory.clear()\n",
        "\n",
        "    while True:\n",
        "        action = choose_action(snake_model, observation)\n",
        "        next_observation, reward, done = env.step(action)\n",
        "        \n",
        "        memory.add_to_memory(observation, action, reward)\n",
        "        \n",
        "        if done:\n",
        "            total_reward = sum(memory.rewards)\n",
        "            \n",
        "            train_step(snake_model, optimizer, \n",
        "                        observations=np.vstack(memory.observations),\n",
        "                        actions=np.array(memory.actions),\n",
        "                        discounted_rewards = discount_rewards(memory.rewards))\n",
        "            \n",
        "            memory.clear()\n",
        "            break\n",
        "        \n",
        "        observation = next_observation"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-56e8ac05c50d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnake_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-0c86f9a3fd30>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetObservations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewardForAction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-0c86f9a3fd30>\u001b[0m in \u001b[0;36mexecuteAction\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnakeHead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnextState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
          ]
        }
      ]
    }
  ]
}